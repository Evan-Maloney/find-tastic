{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "len(query_result)\n",
    "\n",
    "\n",
    "# calculate cosine similarity between string1 and string2\n",
    "\n",
    "\n",
    "string1 = \"This is a test document\"\n",
    "string2 = \"This is a long paragraph\"\n",
    "\n",
    "embed1 = embeddings.embed_query(string1)\n",
    "embed2 = embeddings.embed_query(string2)\n",
    "\n",
    "cosine(embed1, embed2)\n",
    "\n",
    "print(len(embed1))\n",
    "\n",
    "# Now we will use pinecone in langchain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='this is a test document'), 0.459236085)]\n"
     ]
    }
   ],
   "source": [
    "import pinecone   \n",
    "from langchain_community.vectorstores import Pinecone   \n",
    "import numpy as np\n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key='2f9572cf-4cf8-49fc-b48e-4f7d4fc6021f',      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index_name = 'find-documents'\n",
    "index = pinecone.Index(index_name)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, metric='cosine', shards=1, dimension=768)\n",
    "    \n",
    "vector_store = Pinecone(index, embeddings,  \"text\")\n",
    "\n",
    "key = 'aaa'\n",
    "text = \"this is a test document number two\"\n",
    "vector_store.add_texts([text], namespace=key)\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\"two documents\", k=2, ) #namespace=key)\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.33884572982788\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import islice, chain\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(      \n",
    "\tapi_key='2f9572cf-4cf8-49fc-b48e-4f7d4fc6021f',      \n",
    "\tenvironment='gcp-starter'      \n",
    ")   \n",
    "index_name = 'find-documents'\n",
    "\n",
    "# Check if index exists, if not create one\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, metric='cosine', shards=1, dimension=384)\n",
    "\n",
    "# Create an index and embeddings\n",
    "index = pinecone.Index(index_name)\n",
    "embeddings = FastEmbedEmbeddings()\n",
    "vector_store = Pinecone(index, embeddings, \"text\")\n",
    "\n",
    "def ngrams_with_index(input_string, n):\n",
    "    # Join the words into a single string\n",
    "    joined_string = \" \".join(input_string)\n",
    "\n",
    "    # Create n-grams along with start and end character indices\n",
    "    ngrams_with_indices = []\n",
    "    words = joined_string.split()  # Split into words to create n-grams\n",
    "    current_index = 0  # Tracks the current index in the joined_string\n",
    "\n",
    "    for i in range(len(words) - n + 1):\n",
    "        # Create the n-gram\n",
    "        ngram = ' '.join(words[i:i + n])\n",
    "\n",
    "        # Calculate the start index based on the current index\n",
    "        start_idx = current_index\n",
    "\n",
    "        # Calculate the end index\n",
    "        end_idx = start_idx + len(ngram) - 1\n",
    "\n",
    "        # Update current_index for the next iteration\n",
    "        # we only shift one word at a time\n",
    "        current_index += len(words[i]) + 1\n",
    "        ngrams_with_indices.append((ngram, start_idx, end_idx))\n",
    "\n",
    "    return ngrams_with_indices\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Process the document\n",
    "def process_document(document, key):\n",
    "    # Tokenize the document\n",
    "    unigrams = ngrams_with_index(word_tokenize(document), 1)\n",
    "    bigrams = ngrams_with_index(word_tokenize(document), 2)\n",
    "    trigrams = ngrams_with_index(word_tokenize(document), 3)\n",
    "\n",
    "    text_list = []\n",
    "    metadata_list = []\n",
    "    # Add all n-grams to pinecone\n",
    "    for unigram, start, end in unigrams:\n",
    "        text = remove_punctuation(unigram)\n",
    "        if text == '':\n",
    "            continue\n",
    "        text_list.append(text)\n",
    "        metadata_list.append({'start': start, 'end': end})\n",
    "        #vector_store.add_texts([text], namespace=key, metadatas=[{'start': start, 'end': end}])\n",
    "\n",
    "    for bigram, start, end in bigrams:\n",
    "        text = remove_punctuation(bigram)\n",
    "        if len(text.split()) < 2:\n",
    "            continue\n",
    "        text_list.append(text)\n",
    "        metadata_list.append({'start': start, 'end': end})\n",
    "        #vector_store.add_texts([text], namespace=key, metadatas=[{'start': start, 'end': end}])\n",
    "\n",
    "    for trigram, start, end in trigrams:\n",
    "        text = remove_punctuation(trigram)\n",
    "        if len(text.split()) < 3:\n",
    "            continue\n",
    "        text_list.append(text)\n",
    "        metadata_list.append({'start': start, 'end': end})\n",
    "        #vector_store.add_texts([text], namespace=key, metadatas=[{'start': start, 'end': end}])\n",
    "        \n",
    "    #vector_store.add_texts(text_list, namespace=key, batch_size=500)\n",
    "    # convert text_list embeddings\n",
    "    vector_store.add_texts(text_list, namespace=key, batch_size=100, embedding_chunk_size=5000, metadatas=metadata_list)\n",
    "\n",
    "# Example usage\n",
    "import requests\n",
    "document = \"This is a test document. It has several sentences, and it's a good example for n-gram processing.\"*250\n",
    "\n",
    "start = time.time()\n",
    "process_document(document, \"testv1\")\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vector_store.similarity_search_with_score(\"test\", k=2, namespace=\"D1JFQ80WXFYAJLJD4ZI1QKQSWBO0DDBG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fastembed\n",
      "  Downloading fastembed-0.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.65 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from fastembed) (4.66.1)\n",
      "Collecting onnxruntime<2.0,>=1.15\n",
      "  Downloading onnxruntime-1.16.3-cp39-cp39-macosx_11_0_arm64.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.16.0,>=0.15.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from fastembed) (0.15.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from fastembed) (2.31.0)\n",
      "Collecting huggingface-hub==0.19.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx<2.0,>=1.11\n",
      "  Downloading onnx-1.15.0-cp39-cp39-macosx_10_12_universal2.whl (16.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.3 MB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (4.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub==0.19.4->fastembed) (23.2)\n",
      "Collecting protobuf>=3.20.2\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from onnx<2.0,>=1.11->fastembed) (1.24.1)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sympy in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from onnxruntime<2.0,>=1.15->fastembed) (1.12)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (2.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3.0,>=2.31->fastembed) (3.4)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime<2.0,>=1.15->fastembed) (1.3.0)\n",
      "Installing collected packages: humanfriendly, protobuf, huggingface-hub, flatbuffers, coloredlogs, onnxruntime, onnx, fastembed\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "Successfully installed coloredlogs-15.0.1 fastembed-0.1.3 flatbuffers-23.5.26 huggingface-hub-0.19.4 humanfriendly-10.0 onnx-1.15.0 onnxruntime-1.16.3 protobuf-4.25.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete(delete_all=True, namespace=\"testv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Pinecone in module langchain_community.vectorstores.pinecone:\n",
      "\n",
      "class Pinecone(langchain_core.vectorstores.VectorStore)\n",
      " |  Pinecone(index: 'Any', embedding: 'Union[Embeddings, Callable]', text_key: 'str', namespace: 'Optional[str]' = None, distance_strategy: 'Optional[DistanceStrategy]' = <DistanceStrategy.COSINE: 'COSINE'>)\n",
      " |  \n",
      " |  `Pinecone` vector store.\n",
      " |  \n",
      " |  To use, you should have the ``pinecone-client`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.vectorstores import Pinecone\n",
      " |          from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
      " |          import pinecone\n",
      " |  \n",
      " |          # The environment should be the one specified next to the API key\n",
      " |          # in your Pinecone console\n",
      " |          pinecone.init(api_key=\"***\", environment=\"...\")\n",
      " |          index = pinecone.Index(\"langchain-demo\")\n",
      " |          embeddings = OpenAIEmbeddings()\n",
      " |          vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Pinecone\n",
      " |      langchain_core.vectorstores.VectorStore\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, index: 'Any', embedding: 'Union[Embeddings, Callable]', text_key: 'str', namespace: 'Optional[str]' = None, distance_strategy: 'Optional[DistanceStrategy]' = <DistanceStrategy.COSINE: 'COSINE'>)\n",
      " |      Initialize with Pinecone client.\n",
      " |  \n",
      " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, namespace: 'Optional[str]' = None, batch_size: 'int' = 32, embedding_chunk_size: 'int' = 1000, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Upsert optimization is done by chunking the embeddings and upserting them.\n",
      " |      This is done to avoid memory issues and optimize using HTTP based embeddings.\n",
      " |      For OpenAI embeddings, use pool_threads>4 when constructing the pinecone.Index,\n",
      " |      embedding_chunk_size>1000 and batch_size~64 for best performance.\n",
      " |      Args:\n",
      " |          texts: Iterable of strings to add to the vectorstore.\n",
      " |          metadatas: Optional list of metadatas associated with the texts.\n",
      " |          ids: Optional list of ids to associate with the texts.\n",
      " |          namespace: Optional pinecone namespace to add the texts to.\n",
      " |          batch_size: Batch size to use when adding the texts to the vectorstore.\n",
      " |          embedding_chunk_size: Chunk size to use when embedding the texts.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of ids from adding the texts into the vectorstore.\n",
      " |  \n",
      " |  delete(self, ids: 'Optional[List[str]]' = None, delete_all: 'Optional[bool]' = None, namespace: 'Optional[str]' = None, filter: 'Optional[dict]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Delete by vector IDs or filter.\n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          filter: Dictionary of conditions to filter vectors to delete.\n",
      " |  \n",
      " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  max_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  similarity_search(self, query: 'str', k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return pinecone documents most similar to query.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          filter: Dictionary of argument(s) to filter on metadata\n",
      " |          namespace: Namespace to search in. Default will search in '' namespace.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query and score for each\n",
      " |  \n",
      " |  similarity_search_by_vector_with_score(self, embedding: 'List[float]', *, k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None) -> 'List[Tuple[Document, float]]'\n",
      " |      Return pinecone documents most similar to embedding, along with scores.\n",
      " |  \n",
      " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, filter: 'Optional[dict]' = None, namespace: 'Optional[str]' = None) -> 'List[Tuple[Document, float]]'\n",
      " |      Return pinecone documents most similar to query, along with scores.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          filter: Dictionary of argument(s) to filter on metadata\n",
      " |          namespace: Namespace to search in. Default will search in '' namespace.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query and score for each\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_existing_index(index_name: 'str', embedding: 'Embeddings', text_key: 'str' = 'text', namespace: 'Optional[str]' = None, pool_threads: 'int' = 4) -> 'Pinecone' from abc.ABCMeta\n",
      " |      Load pinecone vectorstore from index name.\n",
      " |  \n",
      " |  from_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, batch_size: 'int' = 32, text_key: 'str' = 'text', namespace: 'Optional[str]' = None, index_name: 'Optional[str]' = None, upsert_kwargs: 'Optional[dict]' = None, pool_threads: 'int' = 4, embeddings_chunk_size: 'int' = 1000, **kwargs: 'Any') -> 'Pinecone' from abc.ABCMeta\n",
      " |      Construct Pinecone wrapper from raw documents.\n",
      " |      \n",
      " |      This is a user friendly interface that:\n",
      " |          1. Embeds documents.\n",
      " |          2. Adds the documents to a provided Pinecone index\n",
      " |      \n",
      " |      This is intended to be a quick way to get started.\n",
      " |      \n",
      " |      The `pool_threads` affects the speed of the upsert operations.\n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_community.vectorstores import Pinecone\n",
      " |              from langchain_community.embeddings import OpenAIEmbeddings\n",
      " |              import pinecone\n",
      " |      \n",
      " |              # The environment should be the one specified next to the API key\n",
      " |              # in your Pinecone console\n",
      " |              pinecone.init(api_key=\"***\", environment=\"...\")\n",
      " |              embeddings = OpenAIEmbeddings()\n",
      " |              pinecone = Pinecone.from_texts(\n",
      " |                  texts,\n",
      " |                  embeddings,\n",
      " |                  index_name=\"langchain-demo\"\n",
      " |              )\n",
      " |  \n",
      " |  get_pinecone_index(index_name: 'Optional[str]', pool_threads: 'int' = 4) -> 'Index' from abc.ABCMeta\n",
      " |      Return a Pinecone Index instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name: Name of the index to use.\n",
      " |          pool_threads: Number of threads to use for index upsert.\n",
      " |      Returns:\n",
      " |          Pinecone Index instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |      Access the query embedding object if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async aadd_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |  \n",
      " |  add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async adelete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Optional[bool]'\n",
      " |      Delete by vector ID or other criteria.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          **kwargs: Other keyword arguments that subclasses might use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optional[bool]: True if deletion is successful,\n",
      " |          False otherwise, None if not implemented.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
      " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
      " |      \n",
      " |      Args:\n",
      " |          search_type (Optional[str]): Defines the type of search that\n",
      " |              the Retriever should perform.\n",
      " |              Can be \"similarity\" (default), \"mmr\", or\n",
      " |              \"similarity_score_threshold\".\n",
      " |          search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      " |              search function. Can include things like:\n",
      " |                  k: Amount of documents to return (Default: 4)\n",
      " |                  score_threshold: Minimum relevance threshold\n",
      " |                      for similarity_score_threshold\n",
      " |                  fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      " |                  lambda_mult: Diversity of results returned by MMR;\n",
      " |                      1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      " |                  filter: Filter by document metadata\n",
      " |      \n",
      " |      Returns:\n",
      " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Retrieve more documents with higher diversity\n",
      " |          # Useful if your dataset has many similar documents\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      " |          )\n",
      " |      \n",
      " |          # Fetch more documents for the MMR algorithm to consider\n",
      " |          # But only return the top 5\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
      " |          )\n",
      " |      \n",
      " |          # Only retrieve documents that have a relevance score\n",
      " |          # Above a certain threshold\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"similarity_score_threshold\",\n",
      " |              search_kwargs={'score_threshold': 0.8}\n",
      " |          )\n",
      " |      \n",
      " |          # Only get the single most similar document from the dataset\n",
      " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
      " |      \n",
      " |          # Use a filter to only retrieve documents from a specific paper\n",
      " |          docsearch.as_retriever(\n",
      " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      " |          )\n",
      " |  \n",
      " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query.\n",
      " |  \n",
      " |  async asimilarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |  \n",
      " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1], asynchronously.\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  async asimilarity_search_with_score(self, *args: 'Any', **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with distance asynchronously.\n",
      " |  \n",
      " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  similarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query vector.\n",
      " |  \n",
      " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1].\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async afrom_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  async afrom_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from texts and embeddings.\n",
      " |  \n",
      " |  from_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.7)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.77)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.9)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (1.24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (4.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (3.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (0.1.0)\n",
      "Requirement already satisfied: sentence_transformers in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (2.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.7)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (1.24.1)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.77)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: scipy in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: torchvision in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (0.20.2)\n",
      "Requirement already satisfied: nltk in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: filelock in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.7->langchain) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: click in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pinecone-client in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (2.2.4)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (0.7.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (1.24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from pinecone-client) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->pinecone-client) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in /Users/sheldonroberts/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install langchain sentence_transformers\n",
    "!pip3 install pinecone-client\n",
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformersEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import islice, chain\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(      \n",
    "\tapi_key='2f9572cf-4cf8-49fc-b48e-4f7d4fc6021f',      \n",
    "\tenvironment='gcp-starter'      \n",
    ")   \n",
    "index_name = 'find-documents'\n",
    "\n",
    "# Check if index exists, if not create one\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(name=index_name, metric='cosine', shards=1, dimension=768)\n",
    "\n",
    "# Create an index and embeddings\n",
    "index = pinecone.Index(index_name)\n",
    "embeddings = SentenceTransformersEmbeddings()\n",
    "vector_store = Pinecone(index, embeddings, \"text\")\n",
    "\n",
    "def ngrams_with_index(input_string, n):\n",
    "    # Join the words into a single string\n",
    "    joined_string = \" \".join(input_string)\n",
    "\n",
    "    # Create n-grams along with start and end character indices\n",
    "    ngrams_with_indices = []\n",
    "    words = joined_string.split()  # Split into words to create n-grams\n",
    "    current_index = 0  # Tracks the current index in the joined_string\n",
    "\n",
    "    for i in range(len(words) - n + 1):\n",
    "        # Create the n-gram\n",
    "        ngram = ' '.join(words[i:i + n])\n",
    "\n",
    "        # Calculate the start index based on the current index\n",
    "        start_idx = current_index\n",
    "\n",
    "        # Calculate the end index\n",
    "        end_idx = start_idx + len(ngram) - 1\n",
    "\n",
    "        # Update current_index for the next iteration\n",
    "        # we only shift one word at a time\n",
    "        current_index += len(words[i]) + 1\n",
    "        ngrams_with_indices.append((ngram, start_idx, end_idx))\n",
    "\n",
    "    return ngrams_with_indices\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Process the document\n",
    "def process_document(document, key):\n",
    "    # Tokenize the document\n",
    "    unigrams = ngrams_with_index(word_tokenize(document), 1)\n",
    "    sentence_embeddings = embeddings.embed_documents(sent_tokenize(document))\n",
    "    \n",
    "  \n",
    "    #vector_store.add_texts([text], namespace=key, metadatas=[{'start': start, 'end': end}])\n",
    "        \n",
    "\n",
    "# Example usage\n",
    "import requests\n",
    "document = \"This is a test document. It has several sentences, and it's a good example for n-gram processing.\"*100\n",
    "#document = requests.get(\"https://www.gutenberg.org/cache/epub/35/pg35.txt\").text\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "process_document(document, \"testv1\")\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
